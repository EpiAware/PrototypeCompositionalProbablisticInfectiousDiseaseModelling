---
title: "Case Study 3: Contemporary statistical inference for infectious disease models using Stan"
format: html
---

## Contemporary statistical inference for infectious disease models using Stan

<!-- Reference: https://www.sciencedirect.com/science/article/pii/S1755436519300325 -->

<!-- Implementation: https://cdcgov.github.io/Rt-without-renewal/stable/showcase/replications/chatzilena-2019/ -->

In this vignette, we'll demonstrate how to use `EpiAware` in conjunction with [SciML ecosystem](https://sciml.ai/) for Bayesian inference of infectious disease dynamics.
The model and data is heavily based on [Contemporary statistical inference for infectious disease models using Stan *Chatzilena et al. 2019*](https://www.sciencedirect.com/science/article/pii/S1755436519300325).

We'll cover the following key points:

1.  Defining the deterministic ODE model from Chatzilena et al section 2.2.2 using SciML ODE functionality and an `EpiAware` observation model.
2.  Build on this to define the stochastic ODE model from Chatzilena et al section 2.2.3 using an `EpiAware` observation model.
3.  Fitting the deterministic ODE model to data from an Influenza outbreak in an English boarding school.
4.  Fitting the stochastic ODE model to data from an Influenza outbreak in an English boarding school.

### Packages used in this vignette

Alongside the `EpiAware` package we will use the `OrdinaryDiffEq` and `SciMLSensitivity` packages for interfacing with `SciML` ecosystem; this is a lower dependency usage of `DifferentialEquations.jl` that, respectively, exposes ODE solvers and adjoint methods for ODE solvees; that is the method of propagating parameter derivatives through functions containing ODE solutions.
Bayesian inference will be done with `NUTS` from the `Turing` ecosystem.
We will also use the `CairoMakie` package for plotting and `DataFramesMeta` for data manipulation.

```{julia}
using OrdinaryDiffEqTsit5, OrdinaryDiffEqRosenbrock, SciMLSensitivity #ODE solvers and adjoint methods
using LogExpFunctions #Additional statistics functions

```

### Single population SIR model

As mentioned in *Chatzilena et al* disease spread is frequently modelled in terms of ODE-based models.
The study population is divided into compartments representing a specific stage of the epidemic status.
In this case, susceptible, infected, and recovered individuals.

\begin{align}
{dS \over dt} &= - \beta \frac{I(t)}{N} S(t) \\
{dI \over dt} &= \beta \frac{I(t)}{N} S(t) - \gamma I(t) \\
{dR \over dt} &= \gamma I(t).
\end{align}

where S(t) represents the number of susceptible, I(t) the number of infected and R(t) the number of recovered individuals at time t.
The total population size is denoted by N (with N = S(t) + I(t) + R(t)), β denotes the transmission rate and γ denotes the recovery rate.

We can interface to the `SciML` ecosystem by writing a function with the signature:

> `(du, u, p, t) -> nothing`

Where: - `du` is the *vector field* of the ODE problem, e.g. ${dS \over dt}$, ${dI \over dt}$ etc.
This is calculated *in-place* (commonly denoted using ! in function names in Julia).
- `u` is the *state* of the ODE problem, e.g. $S$, $I$, etc. - `p` is an object that represents the parameters of the ODE problem, e.g. $\beta$, $\gamma$.
- `t` is the time of the ODE problem.

We do this for the SIR model described above in a function called `sir!`:

```{julia}
function sir!(du, u, p, t)
    S, I, R = u
    β, γ = p
    du[1] = -β * I * S
    du[2] = β * I * S - γ * I
    du[3] = γ * I

    return nothing
end
```

We combine vector field function `sir!` with a initial condition `u0` and the integration period `tspan` to make an `ODEProblem`.
We do not define the parameters, these will be defined within an inference approach.

Note that this is analogous to the `EpiProblem` approach we expose from `EpiAware`, as used in the [Mishra et al replication](https://cdcgov.github.io/Rt-without-renewal/dev/showcase/replications/mishra-2020/).
The difference is that here we are going to use ODE solvers from the `SciML` ecosystem to generate the dynamics of the underlying infections.
In the linked example, we use latent process generation exposed by `EpiAware` as the underlying generative process for underlying dynamics.

### Data for inference

There was a brief, but intense, outbreak of Influenza within the (semi-) closed community of a boarding school reported to the British medical journal in 1978.
The outbreak lasted from 22nd January to 4th February and it is reported that one infected child started the epidemic and then it spread rapidly.
Of the 763 children at the boarding scholl, 512 became ill.

We downloaded the data of this outbreak using the R package `outbreaks` which is maintained as part of the [R Epidemics Consortium(RECON)](http://www.%20repidemicsconsortium.org).

```{julia}
data = CSV.read("data/influenza_england_1978_school.csv", DataFrame) |>
              df -> @transform(df,
    :ts=(:date .- minimum(:date)) .|> d -> d.value + 1.0,)

N = 763

sir_prob = ODEProblem(
    sir!,
    N .* [0.99, 0.01, 0.0],
    (0.0, (Date(1978, 2, 4) - Date(1978, 1, 22)).value + 1)
)
```

### Inference for the deterministic SIR model

The boarding school data gives the number of children "in bed" and "convalescent" on each of 14 days from 22nd Jan to 4th Feb 1978.
We follow *Chatzilena et al* and treat the number "in bed" as a proxy for the number of children in the infectious (I) compartment in the ODE model.

The full observation model is:

\begin{align}
Y_t &\sim \text{Poisson}(\lambda_t)\\
\lambda_t &= I(t)\\
\beta &\sim \text{LogNormal}(\text{logmean}=0,\text{logstd}=1) \\
\gamma & \sim \text{Gamma}(\text{shape} = 0.004, \text{scale} = 50)\\
S(0) /N &\sim \text{Beta}(0.5, 0.5).
\end{align}

**NB: Chatzilena et al give** $\lambda_t = \int_0^t  \beta \frac{I(s)}{N} S(s) - \gamma I(s)ds = I(t) - I(0).$ However, this doesn't match their underlying stan code.

From `EpiAware`, we have the `PoissonError` struct which defines the probabilistic structure of this observation error model.

```{julia}
obs = PoissonError()
display(obs)
```

Now we can write the probabilistic model using the `Turing` PPL. Note that instead of using $I(t)$ directly we do the [softplus](https://en.wikipedia.org/wiki/Softplus) transform on $I(t)$ implemented by `LogExpFunctions.log1pexp`.
The reason is that the solver can return small negative numbers, the soft plus transform smoothly maintains positivity which being very close to $I(t)$ when $I(t) > 2$.

```{julia}
@model function deterministic_ode_mdl(y_t, ts, obs, prob, N;
        solver = AutoTsit5(Rosenbrock23())
)
    ##Priors##
    β ~ LogNormal(0.0, 1.0)
    γ ~ Gamma(0.004, 1 / 0.002)
    S₀ ~ Beta(0.5, 0.5)

    ##remake ODE model##
    _prob = remake(prob;
        u0 = [S₀, 1 - S₀, 0.0],
        p = [β, γ]
    )

    ##Solve remade ODE model##

    sol = solve(_prob, solver;
        saveat = ts,
        verbose = false)

    ##log-like accumulation using obs##
    λt = log1pexp.(N * sol[2, :]) # #expected It
    @submodel generated_y_t = generate_observations(obs, y_t, λt)

    ##Generated quantities##
    return (; sol, generated_y_t, R0 = β / γ)
end
```

We instantiate the model in two ways:

1.  `deterministic_mdl`: This conditions the generative model on the data observation. We can sample from this model to find the posterior distribution of the parameters.
2.  `deterministic_uncond_mdl`: This *doesn't* condition on the data. This is useful for prior and posterior predictive modelling.

Here we construct the `Turing` model directly, in the [Mishra et al replication](https://cdcgov.github.io/Rt-without-renewal/dev/showcase/replications/mishra-2020/) we using the `EpiProblem` functionality to build a `Turing` model under the hood.
Because in this note we are using a mix of functionality from `SciML` and `EpiAware`, we construct the model to sample from directly.

```{julia}
deterministic_mdl = deterministic_ode_mdl(data.in_bed, data.ts, obs, sir_prob, N)
deterministic_uncond_mdl = deterministic_ode_mdl(
    fill(missing, length(data.in_bed)), data.ts, obs, sir_prob, N)
```

```{julia}
#| echo: false
function plot_predYt(data, gens; title::String, ylabel::String)
    fig = Figure()
    ga = fig[1, 1:2] = GridLayout()

    ax = Axis(ga[1, 1];
        title = title,
        xticks = (data.ts[1:3:end], data.date[1:3:end] .|> string),
        ylabel = ylabel
    )
    pred_Yt = mapreduce(hcat, gens) do gen
        gen.generated_y_t
    end |> X -> mapreduce(vcat, eachrow(X)) do row
        quantile(row, [0.5, 0.025, 0.975, 0.1, 0.9, 0.25, 0.75])'
    end

    lines!(ax, data.ts, pred_Yt[:, 1]; linewidth = 3, color = :green, label = "Median")
    band!(
        ax, data.ts, pred_Yt[:, 2], pred_Yt[:, 3], color = (:green, 0.2), label = "95% CI")
    band!(
        ax, data.ts, pred_Yt[:, 4], pred_Yt[:, 5], color = (:green, 0.4), label = "80% CI")
    band!(
        ax, data.ts, pred_Yt[:, 6], pred_Yt[:, 7], color = (:green, 0.6), label = "50% CI")
    scatter!(ax, data.in_bed, label = "data")
    leg = Legend(ga[1, 2], ax; framevisible = false)
    hidespines!(ax)

    fig
end
```

**Prior predictive sampling**

```{julia}
prior_chn = sample(deterministic_uncond_mdl, Prior(), 2000)
gens = generated_quantities(deterministic_uncond_mdl, prior_chn)
```

```{julia}
#| output: true
#| echo: false
#| label: fig-prior-pred-det

plot_predYt(data, gens;
    title = "Prior predictive: deterministic model",
    ylabel = "Number of Infected students"
)

```

The prior predictive checking suggests that *a priori* our parameter beliefs are very far from the data.
Approaching the inference naively can lead to poor fits.

We do three things to mitigate this:

1.  We choose a switching ODE solver which switches between explicit (`Tsit5`) and implicit (`Rosenbrock23`) solvers. This helps avoid the ODE solver failing when the sampler tries extreme parameter values. This is the default `solver = AutoTsit5(Rosenbrock23())` above.
2.  We locate the maximum likelihood point, that is we ignore the influence of the priors, as a useful starting point for `NUTS`.

```{julia}
nmle_tries = 100

mle_fit = map(1:nmle_tries) do _
    fit = try
        maximum_likelihood(deterministic_mdl)
    catch
        (lp = -Inf,)
    end
end |>
          fits -> (findmax(fit -> fit.lp, fits)[2], fits) |>
                  max_and_fits -> max_and_fits[2][max_and_fits[1]]

mle_fit.optim_result.retcode
```

Note that we choose the best out of \$nmle_tries tries for the MLE estimators.

Now, we sample aiming at 1000 samples for each of 4 chains.

```{julia}
chn = sample(
    deterministic_mdl, NUTS(), MCMCThreads(), 1000, 4;
    initial_params = fill(mle_fit.values.array, 4)
)

describe(chn)
```

```{julia}
#| output: true
#| echo: false
#| label: fig-pairplot-det
pairplot(chn)
```

**Posterior predictive plotting**

```{julia}
gens = generated_quantities(deterministic_uncond_mdl, chn)
```

```{julia}
#| output: true
#| echo: false
#| label: fig-posterior-pred-det
plot_predYt(data, gens;
    title = "Fitted deterministic model",
    ylabel = "Number of Infected students"
)

```

### Inference for the Stochastic SIR model

In *Chatzilena et al*, they present an auto-regressive model for connecting the outcome of the ODE model to illness observations.
The argument is that the stochastic component of the model can absorb the noise generated by a possible mis-specification of the model.

In their approach they consider $\kappa_t = \log \lambda_t$ where $\kappa_t$ evolves according to an Ornstein-Uhlenbeck process:

$$
d\kappa_t = \phi(\mu_t - \kappa_t) dt + \sigma dB_t.
$$ Which has transition density: $$
\kappa_{t+1} | \kappa_t \sim N\Big(\mu_t + \left(\kappa_t - \mu_t\right)e^{-\phi}, {\sigma^2 \over 2 \phi} \left(1 - e^{-2\phi} \right)\Big).
$$ Where $\mu_t = \log(I(t))$.

We modify this approach since it implies that the $\mu_t$ is treated as constant between observation times.

Instead we redefine $\kappa_t$ as the log-residual:

$\kappa_t = \log(\lambda_t / I(t)).$

With the transition density:

$$
\kappa_{t+1} | \kappa_t \sim N\Big(\kappa_te^{-\phi}, {\sigma^2 \over 2 \phi} \left(1 - e^{-2\phi} \right)\Big).
$$

This is an AR(1) process.

The stochastic model is completed:

\begin{align}
Y_t &\sim \text{Poisson}(\lambda_t)\\
\lambda_t &= I(t)\exp(\kappa_t)\\
\beta &\sim \text{LogNormal}(\text{logmean}=0,\text{logstd}=1) \\
\gamma & \sim \text{Gamma}(\text{shape} = 0.004, \text{scale} = 50)\\
S(0) /N &\sim \text{Beta}(0.5, 0.5)\\
\phi & \sim \text{HalfNormal}(0, 100) \\
1 / \sigma^2 & \sim \text{InvGamma}(0.1,0.1).
\end{align}

We will using the `AR` struct from `EpiAware` to define the auto-regressive process in this model which has a direct parameterisation of the `AR` model.

To convert from the formulation above we sample from the priors, and define `HalfNormal` priors based on the sampled prior means of $e^{-\phi}$ and ${\sigma^2 \over 2 \phi} \left(1 - e^{-2\phi} \right)$.
We also add a strong prior that $\kappa_1 \approx 0$.

```{julia}
ϕs = rand(truncated(Normal(0, 100), lower = 0.0), 1000)
σ²s = rand(InverseGamma(0.1, 0.1), 1000) .|> x -> 1 / x
sampled_AR_damps = ϕs .|> ϕ -> exp(-ϕ)
sampled_AR_stds = map(ϕs, σ²s) do ϕ, σ²
    (1 - exp(-2 * ϕ)) * σ² / (2 * ϕ)
end
```

We define the AR(1) process by matching means of `HalfNormal` prior distributions for the damp parameters and std deviation parameter to the calculated the prior means from the *Chatzilena et al* definition.

```{julia}
ar = AR(
    damp_priors = [HalfNormal(mean(sampled_AR_damps))],
    init_priors = [Normal(0, 0.001)],
    ϵ_t = HierarchicalNormal(std_prior = HalfNormal(mean(sampled_AR_stds)))
)
```

We can sample directly from the behaviour specified by the `ar` struct to do prior predictive checking on the `AR(1)` process.

```{julia}
#| output: true
#| echo: false
#| label: fig-ar-prior-pred
ar_prior_pred_plot = let
    nobs = size(data, 1)
    ar_mdl = generate_latent(ar, nobs)
    fig = Figure()
    ax = Axis(fig[1, 1],
        xticks = (data.ts[1:3:end], data.date[1:3:end] .|> string),
        ylabel = "exp(kt)",
        title = "Prior predictive sampling for relative residual in mean pred."
    )
    for i in 1:500
        lines!(ax, ar_mdl() .|> exp, color = (:grey, 0.15))
    end
    fig
end
ar_prior_pred_plot
```

We see that the choice of priors implies an *a priori* belief that the extra observation noise on the mean prediction of the ODE model is fairly small, approximately 10% relative to the mean prediction.

We can now define the probabilistic model.
The stochastic model assumes a (random) time-varying ascertainment, which we implement using the `Ascertainment` struct from `EpiAware`.
Note that instead of implementing an ascertainment factor `exp.(κₜ)` directly, which can be unstable for large primal values, by default `Ascertainment` uses the `LogExpFunctions.xexpy` function which implements $x\exp(y)$ stabily for a wide range of values.

To distinguish random variables sampled by various sub-processes `EpiAware` process types create prefixes.
The default for `Ascertainment` is just the string `\"Ascertainment\"`, but in this case we use the less verbose `\"va\"` for "varying ascertainment".

```{julia}
mdl_prefix = "va"
```

Now we can construct our time varying ascertianment model.
The main keyword arguments here are `model` and `latent_model`.
`model` sets the connection between the expected observation and the actual observation.
In this case, we reuse our `PoissonError` model from above.
`latent_model` sets the modification model on the expected values.
In this case, we use the `AR` process we defined above.

```{julia}
varying_ascertainment = Ascertainment(
    model = obs,
    latent_model = ar,
    latent_prefix = mdl_prefix
)
```

Now we can declare the full model in the `Turing` PPL.

```{julia}
@model function stochastic_ode_mdl(y_t, ts, obs, prob, N;
        solver = AutoTsit5(Rosenbrock23())
)

    ##Priors##
    β ~ LogNormal(0.0, 1.0)
    γ ~ Gamma(0.004, 1 / 0.002)
    S₀ ~ Beta(0.5, 0.5)

    ##Remake ODE model##
    _prob = remake(prob;
        u0 = [S₀, 1 - S₀, 0.0],
        p = [β, γ]
    )

    ##Solve ODE model##
    sol = solve(_prob, solver;
        saveat = ts,
        verbose = false
    )
    λt = log1pexp.(N * sol[2, :])

    ##Observation##
    @submodel generated_y_t = generate_observations(obs, y_t, λt)

    ##Generated quantities##
    return (; sol, generated_y_t, R0 = β / γ)
end

stochastic_mdl = stochastic_ode_mdl(
    data.in_bed,
    data.ts,
    varying_ascertainment,
    sir_prob,
    N
)

stochastic_uncond_mdl = stochastic_ode_mdl(
    fill(missing, length(data.in_bed)),
    data.ts,
    varying_ascertainment,
    sir_prob,
    N
)
```

**Prior predictive checking**

```{julia}
prior_chn = sample(stochastic_uncond_mdl, Prior(), 2000)
gens = generated_quantities(stochastic_uncond_mdl, prior_chn)
```

```{julia}
#| output: true
#| echo: false
#| label: fig-prior-pred-stoch

plot_predYt(data, gens;
    title = "Prior predictive: stochastic model",
    ylabel = "Number of Infected students"
)

```

The prior predictive checking again shows misaligned prior beliefs; for example *a priori* without data we would not expect the median prediction of number of ill children as about 600 out of \$N after 1 day.

The latent process for the log-residuals $\kappa_t$ doesn't make much sense without priors, so we look for a reasonable MAP point to start NUTS from.
We do this by first making an initial guess which is a mixture of:

1.  The posterior averages from the deterministic model.
2.  The prior averages of the structure parameters of the AR(1) process.
3.  Zero for the time-varying noise underlying the AR(1) process.

```{julia}
initial_guess = [[mean(chn[:β]),
                     mean(chn[:γ]),
                     mean(chn[:S₀]),
                     mean(ar.init_prior)[1],
                     mean(ar.damp_prior)[1],
                     mean(ar.ϵ_t.std_prior)
                 ]
                 zeros(13)]
```

Starting from the initial guess, the MAP point is calculated rapidly in one pass.

```{julia}
map_fit_stoch_mdl = maximum_a_posteriori(stochastic_mdl;
    adtype = AutoReverseDiff(),
    initial_params = initial_guess
)
```

Now we can run NUTS, sampling 1000 posterior draws per chain for 4 chains.

```{julia}
chn2 = sample(
    stochastic_mdl,
    NUTS(; adtype = AutoReverseDiff(true)),
    MCMCThreads(), 1000, 4;
    initial_params = fill(map_fit_stoch_mdl.values.array, 4)
)

describe(chn2)
```

```{julia}
#| output: true
#| echo: false
#| label: fig-pairplot-stoch-params
pairplot(chn2[[:β, :γ, :S₀, Symbol(mdl_prefix * ".std"),
    Symbol(mdl_prefix * ".ar_init[1]"), Symbol(mdl_prefix * ".damp_AR[1]")]])
```

```{julia}
#| output: true
#| echo: false
#| label: fig-pairplot-stoch-eps
vars = mapreduce(vcat, 1:13) do i
    Symbol(mdl_prefix * ".ϵ_t[$i]")
end
pairplot(chn2[vars])
```

```{julia}
gens = generated_quantities(stochastic_uncond_mdl, chn2)
```

```{julia}
#| output: true
#| echo: false
#| label: fig-posterior-pred-stoch
plot_predYt(data, gens;
    title = "Fitted stochastic model",
    ylabel = "Number of Infected students"
)
```